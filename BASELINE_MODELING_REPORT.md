# Baseline Modeling Report - Logistic Regression with Bag-of-Words

## Overview
This report presents the results of baseline modeling using Logistic Regression with Bag-of-Words features across all five sentiment analysis datasets. The baseline establishes performance benchmarks for comparison with more advanced models.

## Methodology

### **Model Configuration:**
- **Algorithm**: Logistic Regression
- **Features**: Bag-of-Words (CountVectorizer)
- **Vocabulary Size**: 5,000 most frequent unigrams
- **Train/Test Split**: 80/20 (stratified)
- **Class Weight**: Balanced (handles class imbalance)
- **Random State**: 42 (reproducible results)

### **Datasets Analyzed:**
1. **Borderlands** - Gaming sentiment (72,218 samples)
2. **Airline** - Customer service sentiment (14,612 samples)
3. **Climate** - Environmental sentiment (43,914 samples)
4. **Corona** - COVID-19 sentiment (44,917 samples)
5. **Combined** - All datasets merged (175,661 samples)

## Performance Results

### **Overall Performance Ranking (by Weighted F1-Score):**

| Rank | Dataset | Test Samples | Accuracy | Weighted F1 | Weighted Precision | Weighted Recall |
|------|---------|--------------|----------|-------------|-------------------|-----------------|
| 1 | **Corona** | 8,984 | 0.7675 | **0.7724** | 0.7860 | 0.7675 |
| 2 | **Borderlands** | 14,444 | 0.7391 | **0.7389** | 0.7449 | 0.7391 |
| 3 | **Airline** | 2,923 | 0.7219 | **0.7332** | 0.7600 | 0.7219 |
| 4 | **Climate** | 8,783 | 0.6822 | **0.6892** | 0.7046 | 0.6822 |
| 5 | **Combined** | 35,133 | 0.6629 | **0.6631** | 0.6643 | 0.6629 |

### **Key Performance Metrics:**
- **Best Performance**: Corona dataset (F1: 0.7724)
- **Worst Performance**: Combined dataset (F1: 0.6631)
- **Average F1-Score**: 0.7193 across all datasets
- **Performance Range**: 0.1092 (significant variation)

## Detailed Analysis by Dataset

### **1. Corona Dataset (Best Performer)**
- **F1-Score**: 0.7724
- **Strengths**: Excellent performance on positive (F1: 0.81) and negative (F1: 0.78) sentiments
- **Weaknesses**: Struggles with neutral sentiment (F1: 0.66)
- **Class Distribution**: Balanced (43.6% positive, 37.9% negative, 18.5% neutral)
- **Most Common Error**: Positive ‚Üí Neutral (507 cases)

### **2. Borderlands Dataset (Second Best)**
- **F1-Score**: 0.7389
- **Strengths**: Consistent performance across all sentiment classes
- **Weaknesses**: Moderate confusion between neutral and positive sentiments
- **Class Distribution**: Neutral-heavy (41.8% neutral, 30.3% negative, 27.9% positive)
- **Most Common Error**: Neutral ‚Üí Positive (1,026 cases)

### **3. Airline Dataset (Third Best)**
- **F1-Score**: 0.7332
- **Strengths**: Excellent negative sentiment detection (F1: 0.81)
- **Weaknesses**: Poor neutral sentiment performance (F1: 0.56)
- **Class Distribution**: Highly imbalanced (62.7% negative, 21.1% neutral, 16.2% positive)
- **Most Common Error**: Negative ‚Üí Neutral (357 cases)

### **4. Climate Dataset (Fourth Best)**
- **F1-Score**: 0.6892
- **Strengths**: Good positive sentiment detection (F1: 0.75)
- **Weaknesses**: Poor negative sentiment performance (F1: 0.45)
- **Class Distribution**: Positive-skewed (52.3% positive, 38.7% neutral, 9.1% negative)
- **Most Common Error**: Positive ‚Üí Neutral (1,050 cases)

### **5. Combined Dataset (Worst Performer)**
- **F1-Score**: 0.6631
- **Strengths**: Balanced overall performance
- **Weaknesses**: Performance degradation due to domain mixing
- **Class Distribution**: Most balanced (37.0% positive, 33.3% neutral, 29.6% negative)
- **Most Common Error**: Positive ‚Üí Neutral (3,085 cases)

## Class Imbalance Impact Analysis

### **Class Imbalance Severity:**
- **Airline**: 62.7% majority class (most imbalanced)
- **Climate**: 52.3% majority class
- **Borderlands**: 41.8% majority class
- **Corona**: 43.6% majority class
- **Combined**: 37.0% majority class (most balanced)

### **Impact on Performance:**
- **High imbalance** (Airline) ‚Üí Lower overall performance despite good negative detection
- **Moderate imbalance** (Climate) ‚Üí Poor minority class (negative) performance
- **Balanced classes** (Combined) ‚Üí More consistent but lower overall performance

## Common Misclassification Patterns

### **Most Frequent Errors:**
1. **Positive ‚Üí Neutral**: Most common across 4/5 datasets
2. **Negative ‚Üí Neutral**: Common in airline dataset
3. **Neutral ‚Üí Positive**: Common in borderlands dataset

### **Pattern Analysis:**
- **Neutral sentiment** is the most challenging to classify correctly
- **Positive sentiment** is often confused with neutral
- **Negative sentiment** is generally well-detected (except in climate dataset)

## Baseline Model Strengths

### **‚úÖ Advantages:**
1. **Simplicity**: Easy to understand and interpret
2. **Speed**: Fast training and prediction (2-3 seconds per dataset)
3. **Stability**: Consistent convergence across all datasets
4. **Class Balance**: Handles imbalance with balanced class weights
5. **Baseline Performance**: Reasonable performance (66-77% F1-score)

## Baseline Model Weaknesses

### **‚ùå Limitations:**
1. **Bag-of-Words Limitation**: No word order or context consideration
2. **Vocabulary Constraint**: Limited to 5,000 most frequent words
3. **Class Imbalance Sensitivity**: Struggles with highly imbalanced datasets
4. **Neutral Sentiment Confusion**: Consistently poor neutral sentiment performance
5. **Domain Mixing**: Performance degrades when combining different domains

## Improvement Opportunities

### **üöÄ Next Steps for Advanced Models:**

#### **Feature Engineering:**
1. **TF-IDF**: Replace raw counts with TF-IDF weighting
2. **N-grams**: Include bigrams and trigrams for context
3. **Text Length Features**: Add word count and character count
4. **Domain-Specific Features**: Extract domain-specific vocabulary

#### **Model Improvements:**
1. **SVM**: Support Vector Machine with RBF kernel
2. **Random Forest**: Ensemble method for better generalization
3. **Neural Networks**: Deep learning for complex patterns
4. **Word Embeddings**: Word2Vec, GloVe, or FastText

#### **Advanced Techniques:**
1. **Ensemble Methods**: Combine multiple models
2. **Domain Adaptation**: Transfer learning between domains
3. **Hyperparameter Tuning**: Optimize model parameters
4. **Cross-Validation**: More robust evaluation

## Technical Implementation Details

### **Training Statistics:**
- **Total Training Time**: ~6.5 seconds across all datasets
- **Convergence**: All models converged within 88-217 iterations
- **Memory Usage**: Efficient with sparse matrices
- **Reproducibility**: Fixed random seeds ensure consistent results

### **Evaluation Methodology:**
- **Stratified Splits**: Maintains class distribution in train/test sets
- **Weighted Metrics**: Accounts for class imbalance
- **Confusion Matrices**: Visual analysis of misclassification patterns
- **Per-Class Metrics**: Detailed performance breakdown

## Files Generated

### **Per Dataset (5 datasets √ó 2 files = 10 files):**
- `classification_report.txt` - Detailed performance metrics
- `confusion_matrix.png` - Visual confusion matrix

### **Summary Files:**
- `baseline_summary_table.csv` - Performance comparison table
- `baseline_summary.txt` - Formatted performance summary
- `baseline_insights.txt` - Detailed analysis and insights

## Conclusion

The baseline modeling successfully established performance benchmarks across all five datasets. Key findings include:

1. **Domain-Specific Performance**: Corona dataset performs best (F1: 0.7724), while combined dataset performs worst (F1: 0.6631)
2. **Class Imbalance Impact**: Highly imbalanced datasets (Airline) show performance degradation
3. **Neutral Sentiment Challenge**: Consistently the most difficult sentiment to classify
4. **Room for Improvement**: 10-15% performance improvement possible with advanced techniques

This baseline provides a solid foundation for comparing more sophisticated models and validates the need for domain-specific approaches and advanced feature engineering techniques.

---

**Total Analysis Time**: ~6.5 seconds  
**Files Generated**: 13 files (10 per-dataset + 3 summary files)  
**Key Finding**: 10.9% performance variation across domains - strong justification for domain-specific modeling
